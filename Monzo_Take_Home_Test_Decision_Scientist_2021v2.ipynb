{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Wgl_sAgdbG7"
   },
   "source": [
    "Thanks for taking your time to complete this take-home task. \n",
    "\n",
    "This is where you can show off all your decision science skills through a hands-on project. Together with the dataset, this notebook presents a series of typical problems we face in our job. We expect you to complete the notebook with insightful analysis, elegant code and clear documentation of your findings (perfectly OK to embed within this notebook, just make sure using a distinct color). Please also feel free to keep in the final notebook any code+result where you believe there is value, even if it's not directly addressing the questions below.\n",
    "\n",
    "We have designed this test with the hope to only take you about 3 hours. We know itâ€™s still quite a commitment of your time and we really appreciate it. That being said, this test is not timed so feel free to spend more time if needed. Weâ€™d love to hear your feedback if it actually cost you longer than we intended. It is easy for us to underestimate the required efforts when we know the answers already! Please also tell us if you havenâ€™t had enough fun ðŸ˜‚\n",
    "\n",
    "\n",
    "## Hints\n",
    "\n",
    "*   We encourage our decision scientists to use open-source solutions as much as possible instead of re-inventing the wheels. So please feel free to Google for a solution before writing any complicated code. \n",
    "*   If you are running short of time but have lots of extra great ideas you'd like to explore, feel free to write down your thoughts in words or pseudo-code instead of code.\n",
    "\n",
    "## Context\n",
    "\n",
    "The project we have here is to validate and monitor two credit risk decision models for personal loan underwriting decisions. The information we collected at the underwriting stage of successful applications (i.e. approved and disbursed) were used as the input features of these two models. The two models were trained to predict whether the customers will repay all scheduled payment in the following X months. \n",
    "\n",
    "We can split our data into the following three parts:\n",
    "1.   the **development sample**: all loans applications before 1st August 2019\n",
    "2.   the **out-of-time monitoring sample**: all loans applications between 1st August 2019 and 1st Jan 2020\n",
    "3.   the **post-deployment monitoring sample**: all loans applications since 1st Jan 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh-P4EKIalTG"
   },
   "source": [
    "# Part 1. Data processing\n",
    "\n",
    "Here we have two synthetic datasets of loan customers. \n",
    "\n",
    "\n",
    "The file ***monthly_outcome.csv*** contains the repayment *status* at each scheduled payment *date*. The numeric values in column *status* represent how many monthly repayments they were missing. i.e. 0 means they were up to date without any arrear, 3 means they were at that time missing three repayments. Once a customer reaches a status of missing 4 payments, we marked them as 'D' which means it has defaulted. For simplicity let's assume it as a terminal status and all the following records will be 'D' as well. \n",
    "\n",
    "The file ***application.csv*** contains\n",
    "1.   a few variables about the customer we acquired during their loan applications\n",
    "2.   a binary variable (*is_bad_12m*) of their repayment performance 12 months after disbursal \n",
    "\n",
    "**Target definition**: the binary target variable *is_bad_12m* was created by looking at the arrear status of the customers across the 12 dates where the repayment was scheduled. If at any point they had 3 or more repayments missing, this variable would be set to 1, otherwise 0. \n",
    "\n",
    "Each loan is represented by an *unique_id* in these two files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vT7729qjNAT"
   },
   "source": [
    "## Task 1.1: data cleaning\n",
    "\n",
    "Although we are proud of our data infrastructure, it's always a good practice to sense check and clean up a bit. We have left you a few surprises here. Have fun!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "outcomes_df = pd.read_csv('monthly_outcome.csv')\n",
    "application_df = pd.read_csv('application.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Clearning Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>date</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39489</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65413</td>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id        date status\n",
       "0      39489  2018-02-02      0\n",
       "1      65413  2018-02-02      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "0.0    470201\n",
       "0      324703\n",
       "D       16074\n",
       "1.0     15524\n",
       "1       10592\n",
       "2.0      5259\n",
       "2        3141\n",
       "3.0      1296\n",
       "3        1190\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Status contains expected values although some formatting differences in values eg. 0.0 vs 0\n",
    "outcomes_df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all numerical values in status integers eg. force 0.0 to 0\n",
    "outcomes_df.status = outcomes_df.status.map(lambda x: x if x=='D' else round(float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "0    794904\n",
       "1     26116\n",
       "D     16074\n",
       "2      8400\n",
       "3      2486\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check formatting again\n",
    "outcomes_df.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id\n",
       "39489    12\n",
       "31197    12\n",
       "63209    12\n",
       "84772    12\n",
       "27745    12\n",
       "         ..\n",
       "10739    12\n",
       "3603     12\n",
       "79401    12\n",
       "83348    12\n",
       "44277    12\n",
       "Name: count, Length: 70665, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All ids appear 12 times\n",
    "outcomes_df.unique_id.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-03-23    1602\n",
       "2020-03-15    1593\n",
       "2020-04-11    1583\n",
       "2020-04-01    1563\n",
       "2020-03-22    1563\n",
       "              ... \n",
       "2018-02-08       1\n",
       "2018-02-17       1\n",
       "2018-02-20       1\n",
       "2018-02-06       1\n",
       "2018-02-05       1\n",
       "Name: count, Length: 1232, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes_df.date.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dates appear valid dates as no errors raised here\n",
    "outcomes_df.date = pd.to_datetime(outcomes_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id    False\n",
       "date         False\n",
       "status       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No missing values\n",
    "outcomes_df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if arrear status is properly set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'fake' status column where D is valued at 4 fo easier comparisons later\n",
    "outcomes_df['fake_status'] = outcomes_df.status.map(lambda x: 4 if x=='D' else round(float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column to look at last status of the loan\n",
    "outcomes_df = outcomes_df.sort_values([\"unique_id\", \"date\"])\n",
    "\n",
    "outcomes_df[\"prev_status\"] = outcomes_df.groupby(\"unique_id\")[\"fake_status\"].shift(1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any occurances of a loan changing from defaulted to non-defaulted later\n",
    "# Looks like no - which is good!\n",
    "outcomes_df['valid_prev_status'] = outcomes_df.fake_status >= outcomes_df.prev_status\n",
    "outcomes_df[(outcomes_df.valid_prev_status == False) & (outcomes_df.prev_status == 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Cleaning Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>stress_score</th>\n",
       "      <th>is_bad_12m</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "      <th>origination_date</th>\n",
       "      <th>loan_term</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>age_oldest_account</th>\n",
       "      <th>total_value_of_mortgage</th>\n",
       "      <th>current_utilisation</th>\n",
       "      <th>months_since_2_payments_missed</th>\n",
       "      <th>number_of_credit_searches_last_3_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.064978</td>\n",
       "      <td>0.044780</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3484.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-999997.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006730</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>191842.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-999997.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id  stress_score  is_bad_12m   model_1   model_2 origination_date  \\\n",
       "0      19550           NaN         1.0  0.064978  0.044780       2019-08-31   \n",
       "1      37749           NaN         0.0  0.006730  0.002496       2019-03-20   \n",
       "\n",
       "   loan_term  loan_amount  age_oldest_account  total_value_of_mortgage  \\\n",
       "0       24.0       3484.0                68.0                -999997.0   \n",
       "1       36.0       5510.0               177.0                 191842.0   \n",
       "\n",
       "   current_utilisation  months_since_2_payments_missed  \\\n",
       "0                366.0                            15.0   \n",
       "1                 72.0                       -999997.0   \n",
       "\n",
       "   number_of_credit_searches_last_3_months  \n",
       "0                                      0.0  \n",
       "1                                      1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id                                    int64\n",
       "stress_score                               float64\n",
       "is_bad_12m                                 float64\n",
       "model_1                                    float64\n",
       "model_2                                    float64\n",
       "origination_date                            object\n",
       "loan_term                                  float64\n",
       "loan_amount                                float64\n",
       "age_oldest_account                         float64\n",
       "total_value_of_mortgage                    float64\n",
       "current_utilisation                        float64\n",
       "months_since_2_payments_missed             float64\n",
       "number_of_credit_searches_last_3_months    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id                                  False\n",
       "stress_score                                True\n",
       "is_bad_12m                                  True\n",
       "model_1                                    False\n",
       "model_2                                    False\n",
       "origination_date                           False\n",
       "loan_term                                  False\n",
       "loan_amount                                False\n",
       "age_oldest_account                         False\n",
       "total_value_of_mortgage                    False\n",
       "current_utilisation                        False\n",
       "months_since_2_payments_missed             False\n",
       "number_of_credit_searches_last_3_months    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values. Expected some missing values of st\n",
    "application_df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg4QMTo2czc4"
   },
   "source": [
    "## Task 1.2: another target\n",
    "In order to understand the performance of a model before it's too late, we also want to monitor the repayment behaviours after the first few repayments. \n",
    "\n",
    "Could you please create another \"early-risk\" target *is_bad_3m* which represents whether the customers ever had **2 or more** repayments in arrear at any point of their first three scheduled ones?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd1jByeiY7bP"
   },
   "source": [
    "# Part 2. Model validation\n",
    "\n",
    "In this part let's assume we are still at the model development stage and look at the development sample only (see definition at the start). We will skip the model training part here (which is too much fun to finish in 3 hours), and assume that we already trained two candidate models. These are of course probabilistic classification model, which you can find their scores in ***application.csv*** as columns *model_1* and *model_2*. \n",
    "\n",
    "We need to compare their performance and decide which one to use in production. The winner model, once deployed, will be used for decisions of\n",
    "\n",
    "*   Loan approval: the score must be above certain threshold (which can be adjusted during operation) for the application to be approved. \n",
    "*   Loss estimate: for each approved loan, we use the model output to predict the probability of default. \n",
    "*   Pricing: based on the loss estimate, we decide the interest rate to be charged in order to cover potential losses. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjBbC6y1Ca8c"
   },
   "source": [
    "\n",
    "\n",
    "## Task 2.1: classification power\n",
    "\n",
    "A common metric used in the credit risk modelling world is the Gini coefficient, which can be linearly mapped to ROCAUC if that's a term you are more familiar with. Could you please compare the Gini's between the two models as a first step? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSTL7anX0sv8"
   },
   "source": [
    "An extended question: assuming that classification power is all we care about, what are the other reasons to not pick the model with highest Gini? It's enough to just write down your thoughts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gDBOJwwCdiS"
   },
   "source": [
    "## Task 2.2: classification power in segments\n",
    "\n",
    "As the population of future business might have different distributions from the development sample, we would ideally want the chosen model to be performant in all segments. For simplicity let's stick with univariate segments only.\n",
    "\n",
    "Could you please compare the Gini's between the two models in the segments of all the variables? Feel free to define the segments as you see appropriate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgdjGEh1Cfno"
   },
   "source": [
    "\n",
    "## Task 2.3: accuracy \n",
    "As we want to use our model for loss estimates and pricing of each customer, could you please check whether the scores (as probabilistic predictions) are accurate with respect to the actual \"bad rates\" (i.e. the fraction of *is_bad_12m*=1 among customers of similar scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrTIJak3DCqW"
   },
   "source": [
    "## Task 2.4: calibration\n",
    "\n",
    "We also want to monitor the early risk indicator *is_bad_3m* in case something really bad happens (e.g. a pandemic). For that we need to calibrate our scores to the probability of such short-term outcome. Could you please create the calibrated scores for the two models and validate them? (Hint: if this is not a topic you are familiar with, scikit-learn has some handy utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uz8lFvDwECZF"
   },
   "source": [
    "# Part 3. Model monitoring\n",
    "\n",
    "The training and validation of a model is just part of the story. A large part of our work is to understand how our models perform in real life deicisioning and how we adapt to the changing market. In this part we will look into the monitoring sample (see definition at the start).\n",
    "\n",
    "Now let's assume that we have choosen *model_1* and deployed it to production since 1st Jan 2020. On that day, our decision engine started to use that model, and since then only approved applications with *model_1*<0.05. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Mh84z4QEDAu"
   },
   "source": [
    "## Task 3.1: model performance\n",
    "\n",
    "How did the model perform in this monitoring sample, before & after its deployment into production? How does that compare to the expectation from the development sample? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuS5S6Q__l6-"
   },
   "source": [
    "## Task 3.2: why the changes?\n",
    "\n",
    "If you observe a difference, what do you think can be the reason(s)? How are you going to verify your hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEiF96SWEDDE"
   },
   "source": [
    "## Task 3.3: new variable\n",
    "\n",
    "You might have noticed that a new variable ***stress_score*** has become available since late 2019. Can you figure out whether there is additional classification power from this variable over our models? \n",
    "\n",
    "If so, how would you incorporate it into our decision model?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Monzo_Take_Home_Test_Decision_Scientist.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
